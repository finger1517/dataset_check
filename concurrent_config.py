#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
å¹¶å‘é…ç½®ä¼˜åŒ–æ¨¡å—
ä¸ºä¸åŒçš„æ•°æ®é›†æ¡†æ¶æä¾›æœ€ä½³çš„å¹¶å‘é…ç½®
"""

import os
from multiprocessing import cpu_count
from typing import Dict, Tuple

def get_optimal_config() -> Dict:
    """è·å–ç³»ç»Ÿæœ€ä¼˜å¹¶å‘é…ç½®"""
    num_cpus = cpu_count()
    
    # åŸºäºCPUæ ¸å¿ƒæ•°ç¡®å®šæœ€ä¼˜é…ç½®
    if num_cpus >= 16:
        # é«˜æ€§èƒ½æœåŠ¡å™¨
        config = {
            'pytorch_workers': 8,
            'ray_workers': 12,
            'hf_processes': 8,
            'batch_sizes': [8, 16, 32],
            'ray_batch_size': 16,
            'ray_num_cpus_per_task': 2,
            'ray_concurrency': 8
        }
    elif num_cpus >= 8:
        # ä¸­ç­‰æ€§èƒ½æœºå™¨
        config = {
            'pytorch_workers': 6,
            'ray_workers': 8,
            'hf_processes': 6,
            'batch_sizes': [4, 8, 16],
            'ray_batch_size': 12,
            'ray_num_cpus_per_task': 2,
            'ray_concurrency': 6
        }
    elif num_cpus >= 4:
        # æ™®é€šæœºå™¨
        config = {
            'pytorch_workers': 4,
            'ray_workers': 4,
            'hf_processes': 4,
            'batch_sizes': [4, 8],
            'ray_batch_size': 8,
            'ray_num_cpus_per_task': 1,
            'ray_concurrency': 4
        }
    else:
        # ä½æ€§èƒ½æœºå™¨
        config = {
            'pytorch_workers': 2,
            'ray_workers': 2,
            'hf_processes': 2,
            'batch_sizes': [2, 4],
            'ray_batch_size': 4,
            'ray_num_cpus_per_task': 1,
            'ray_concurrency': 2
        }
    
    config['num_cpus'] = num_cpus
    return config

def get_pytorch_dataloader_config(num_workers: int = None) -> Dict:
    """è·å–PyTorch DataLoaderçš„æœ€ä¼˜é…ç½®"""
    config = get_optimal_config()
    
    if num_workers is None:
        num_workers = config['pytorch_workers']
    
    return {
        'num_workers': num_workers,
        'pin_memory': True,  # å¦‚æœæœ‰GPUå¯ç”¨
        'persistent_workers': True if num_workers > 0 else False,
        'prefetch_factor': 4 if num_workers > 0 else 2,
        'drop_last': False,
        'timeout': 60  # 60ç§’è¶…æ—¶
    }

def get_ray_config(num_workers: int = None) -> Dict:
    """è·å–Rayæ•°æ®é›†çš„æœ€ä¼˜é…ç½®"""
    config = get_optimal_config()
    
    if num_workers is None:
        num_workers = config['ray_workers']
    
    return {
        'batch_size': config['ray_batch_size'],
        'num_cpus': config['ray_num_cpus_per_task'],
        'concurrency': min(num_workers, config['ray_concurrency']),
        'max_concurrent_tasks': num_workers
    }

def get_hf_config(num_proc: int = None) -> Dict:
    """è·å–HuggingFaceæ•°æ®é›†çš„æœ€ä¼˜é…ç½®"""
    config = get_optimal_config()
    
    if num_proc is None:
        num_proc = config['hf_processes']
    
    return {
        'num_proc': num_proc,
        'batch_size': 1000,  # HFå†…éƒ¨æ‰¹å¤„ç†å¤§å°
        'writer_batch_size': 1000,
        'keep_in_memory': False,  # å¯¹äºå¤§æ•°æ®é›†ï¼Œä¸ä¿å­˜åœ¨å†…å­˜ä¸­
        'load_from_cache_file': True
    }

def print_system_info():
    """æ‰“å°ç³»ç»Ÿä¿¡æ¯å’Œæ¨èé…ç½®"""
    config = get_optimal_config()
    
    print("ğŸ’» ç³»ç»Ÿä¿¡æ¯å’Œæ¨èé…ç½®:")
    print(f"   CPUæ ¸å¿ƒæ•°: {config['num_cpus']}")
    print(f"   PyTorch workers: {config['pytorch_workers']}")
    print(f"   Ray workers: {config['ray_workers']}")
    print(f"   HuggingFace processes: {config['hf_processes']}")
    print(f"   æ¨èæ‰¹å¤„ç†å¤§å°: {config['batch_sizes']}")
    print()

if __name__ == "__main__":
    print_system_info()
    
    print("PyTorch DataLoaderé…ç½®:")
    pytorch_config = get_pytorch_dataloader_config()
    for key, value in pytorch_config.items():
        print(f"   {key}: {value}")
    
    print("\nRayæ•°æ®é›†é…ç½®:")
    ray_config = get_ray_config()
    for key, value in ray_config.items():
        print(f"   {key}: {value}")
    
    print("\nHuggingFaceæ•°æ®é›†é…ç½®:")
    hf_config = get_hf_config()
    for key, value in hf_config.items():
        print(f"   {key}: {value}") 