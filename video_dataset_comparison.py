#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†é¢‘æ•°æ®é›†å¤„ç†æ•ˆèƒ½æ¯”è¾ƒè„šæœ¬
æ¯”è¾ƒRayã€HuggingFaceå’ŒPyTorchæ•°æ®é›†åœ¨å¤„ç†è§†é¢‘æ•°æ®æ—¶çš„æ€§èƒ½
"""

import os
import time
import glob
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import ray
from ray.data import Dataset as RayDataset
from datasets import Dataset as HFDataset
import pandas as pd
from typing import List, Tuple, Dict
import psutil
import gc
from pathlib import Path

# è®¾ç½®è§†é¢‘æ•°æ®è·¯å¾„
VIDEO_DATA_PATH = "../video_data"

def find_all_mp4_files(base_path: str) -> List[str]:
    """é€’å½’æŸ¥æ‰¾æ‰€æœ‰mp4æ–‡ä»¶"""
    mp4_files = []
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.mp4'):
                mp4_files.append(os.path.join(root, file))
    return mp4_files

def extract_frames_from_video(video_path: str, num_frames: int = 16, target_size: Tuple[int, int] = (224, 224)) -> np.ndarray:
    """ä»è§†é¢‘ä¸­å‡åŒ€æå–æŒ‡å®šæ•°é‡çš„å¸§å¹¶è°ƒæ•´å¤§å°"""
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if total_frames == 0:
        cap.release()
        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)
    
    # è®¡ç®—å‡åŒ€é‡‡æ ·çš„å¸§ç´¢å¼•
    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    frames = []
    
    for frame_idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if ret:
            # è°ƒæ•´å¤§å°å¹¶è½¬æ¢é¢œè‰²ç©ºé—´
            frame = cv2.resize(frame, target_size)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
        else:
            # å¦‚æœè¯»å–å¤±è´¥ï¼Œæ·»åŠ é›¶å¸§
            frames.append(np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8))
    
    cap.release()
    return np.array(frames)

# ==================== PyTorch Dataset å®ç° ====================
class PyTorchVideoDataset(Dataset):
    """PyTorchæ ‡å‡†æ•°æ®é›†å®ç°"""
    
    def __init__(self, video_paths: List[str], num_frames: int = 16, target_size: Tuple[int, int] = (224, 224)):
        self.video_paths = video_paths
        self.num_frames = num_frames
        self.target_size = target_size
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def __len__(self):
        return len(self.video_paths)
    
    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        frames = extract_frames_from_video(video_path, self.num_frames, self.target_size)
        
        # è½¬æ¢ä¸ºtensorå¹¶åº”ç”¨æ ‡å‡†åŒ–
        frames_tensor = torch.stack([self.transform(frame) for frame in frames])
        
        return {
            'frames': frames_tensor,
            'video_path': video_path,
            'video_name': os.path.basename(video_path)
        }

# ==================== Ray Dataset å®ç° ====================
def process_video_ray(batch: Dict) -> Dict:
    """Rayæ•°æ®å¤„ç†å‡½æ•°"""
    video_paths = batch['video_path']
    processed_batch = {
        'frames': [],
        'video_path': [],
        'video_name': []
    }
    
    for video_path in video_paths:
        frames = extract_frames_from_video(video_path, 16, (224, 224))
        # æ ‡å‡†åŒ–å¤„ç†
        frames = frames.astype(np.float32) / 255.0
        frames = (frames - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
        
        processed_batch['frames'].append(frames)
        processed_batch['video_path'].append(video_path)
        processed_batch['video_name'].append(os.path.basename(video_path))
    
    return processed_batch

def create_ray_dataset(video_paths: List[str]) -> RayDataset:
    """åˆ›å»ºRayæ•°æ®é›†"""
    data = [{'video_path': path} for path in video_paths]
    ds = ray.data.from_items(data)
    return ds.map_batches(process_video_ray, batch_size=4)

# ==================== HuggingFace Dataset å®ç° ====================
def process_video_hf(example):
    """HuggingFaceæ•°æ®å¤„ç†å‡½æ•°"""
    video_path = example['video_path']
    frames = extract_frames_from_video(video_path, 16, (224, 224))
    
    # æ ‡å‡†åŒ–å¤„ç†
    frames = frames.astype(np.float32) / 255.0
    frames = (frames - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
    
    return {
        'frames': frames,
        'video_path': video_path,
        'video_name': os.path.basename(video_path)
    }

def create_hf_dataset(video_paths: List[str]) -> HFDataset:
    """åˆ›å»ºHuggingFaceæ•°æ®é›†"""
    data = [{'video_path': path} for path in video_paths]
    dataset = HFDataset.from_list(data)
    return dataset.map(process_video_hf, num_proc=4)

# ==================== æ€§èƒ½æµ‹è¯•å‡½æ•° ====================
def measure_memory_usage():
    """æµ‹é‡å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB

def benchmark_pytorch_dataset(video_paths: List[str], batch_size: int = 4) -> Dict:
    """æµ‹è¯•PyTorchæ•°æ®é›†æ€§èƒ½"""
    print("ğŸ”¥ æµ‹è¯•PyTorchæ•°æ®é›†...")
    
    start_memory = measure_memory_usage()
    start_time = time.time()
    
    dataset = PyTorchVideoDataset(video_paths)
    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=False, num_workers=2)
    
    processed_samples = 0
    for batch in dataloader:
        processed_samples += len(batch['frames'])
        # æ¨¡æ‹Ÿä¸€äº›å¤„ç†
        _ = batch['frames'].mean()
    
    end_time = time.time()
    end_memory = measure_memory_usage()
    
    return {
        'name': 'PyTorch Dataset',
        'total_time': end_time - start_time,
        'samples_processed': processed_samples,
        'samples_per_second': processed_samples / (end_time - start_time),
        'memory_usage_mb': end_memory - start_memory,
        'peak_memory_mb': end_memory
    }

def benchmark_ray_dataset(video_paths: List[str], batch_size: int = 4) -> Dict:
    """æµ‹è¯•Rayæ•°æ®é›†æ€§èƒ½"""
    print("âš¡ æµ‹è¯•Rayæ•°æ®é›†...")
    
    start_memory = measure_memory_usage()
    start_time = time.time()
    
    dataset = create_ray_dataset(video_paths)
    
    processed_samples = 0
    for batch in dataset.iter_batches(batch_size=batch_size):
        processed_samples += len(batch['frames'])
        # æ¨¡æ‹Ÿä¸€äº›å¤„ç†
        _ = np.mean([np.mean(frames) for frames in batch['frames']])
    
    end_time = time.time()
    end_memory = measure_memory_usage()
    
    return {
        'name': 'Ray Dataset',
        'total_time': end_time - start_time,
        'samples_processed': processed_samples,
        'samples_per_second': processed_samples / (end_time - start_time),
        'memory_usage_mb': end_memory - start_memory,
        'peak_memory_mb': end_memory
    }

def benchmark_hf_dataset(video_paths: List[str], batch_size: int = 4) -> Dict:
    """æµ‹è¯•HuggingFaceæ•°æ®é›†æ€§èƒ½"""
    print("ğŸ¤— æµ‹è¯•HuggingFaceæ•°æ®é›†...")
    
    start_memory = measure_memory_usage()
    start_time = time.time()
    
    dataset = create_hf_dataset(video_paths)
    
    processed_samples = 0
    for i in range(0, len(dataset), batch_size):
        batch = dataset[i:i+batch_size]
        if isinstance(batch['frames'], list):
            processed_samples += len(batch['frames'])
            # æ¨¡æ‹Ÿä¸€äº›å¤„ç†
            _ = np.mean([np.mean(frames) for frames in batch['frames']])
        else:
            processed_samples += 1
            _ = np.mean(batch['frames'])
    
    end_time = time.time()
    end_memory = measure_memory_usage()
    
    return {
        'name': 'HuggingFace Dataset',
        'total_time': end_time - start_time,
        'samples_processed': processed_samples,
        'samples_per_second': processed_samples / (end_time - start_time),
        'memory_usage_mb': end_memory - start_memory,
        'peak_memory_mb': end_memory
    }

def print_results(results: List[Dict]):
    """æ‰“å°æµ‹è¯•ç»“æœ"""
    print("\n" + "="*80)
    print("ğŸ“Š è§†é¢‘æ•°æ®é›†å¤„ç†æ€§èƒ½æ¯”è¾ƒç»“æœ")
    print("="*80)
    
    for result in results:
        print(f"\nğŸ¯ {result['name']}:")
        print(f"   æ€»å¤„ç†æ—¶é—´: {result['total_time']:.2f} ç§’")
        print(f"   å¤„ç†æ ·æœ¬æ•°: {result['samples_processed']}")
        print(f"   å¤„ç†é€Ÿåº¦: {result['samples_per_second']:.2f} æ ·æœ¬/ç§’")
        print(f"   å†…å­˜ä½¿ç”¨: {result['memory_usage_mb']:.2f} MB")
        print(f"   å³°å€¼å†…å­˜: {result['peak_memory_mb']:.2f} MB")
    
    # æ‰¾å‡ºæœ€å¿«çš„æ–¹æ³•
    fastest = min(results, key=lambda x: x['total_time'])
    most_efficient_memory = min(results, key=lambda x: x['memory_usage_mb'])
    
    print(f"\nğŸ† æœ€å¿«å¤„ç†: {fastest['name']} ({fastest['samples_per_second']:.2f} æ ·æœ¬/ç§’)")
    print(f"ğŸ’¾ æœ€çœå†…å­˜: {most_efficient_memory['name']} ({most_efficient_memory['memory_usage_mb']:.2f} MB)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ å¼€å§‹è§†é¢‘æ•°æ®é›†å¤„ç†æ•ˆèƒ½æ¯”è¾ƒæµ‹è¯•...")
    
    # åˆå§‹åŒ–Ray
    if not ray.is_initialized():
        ray.init(ignore_reinit_error=True)
    
    # æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘æ–‡ä»¶
    print("ğŸ“ æ­£åœ¨æœç´¢è§†é¢‘æ–‡ä»¶...")
    video_paths = find_all_mp4_files(VIDEO_DATA_PATH)
    
    if not video_paths:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•mp4æ–‡ä»¶ï¼")
        return
    
    # é™åˆ¶æµ‹è¯•æ–‡ä»¶æ•°é‡ä»¥ä¾¿å¿«é€Ÿæµ‹è¯•
    test_video_paths = video_paths[:min(200, len(video_paths))]
    print(f"ğŸ“¹ æ‰¾åˆ° {len(video_paths)} ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œå°†æµ‹è¯•å‰ {len(test_video_paths)} ä¸ª")
    
    results = []
    batch_size = 4
    
    try:
        # æµ‹è¯•PyTorchæ•°æ®é›†
        gc.collect()
        pytorch_result = benchmark_pytorch_dataset(test_video_paths, batch_size)
        results.append(pytorch_result)
        
        # æµ‹è¯•Rayæ•°æ®é›†
        gc.collect()
        ray_result = benchmark_ray_dataset(test_video_paths, batch_size)
        results.append(ray_result)
        
        # æµ‹è¯•HuggingFaceæ•°æ®é›†
        gc.collect()
        hf_result = benchmark_hf_dataset(test_video_paths, batch_size)
        results.append(hf_result)
        
        # æ‰“å°ç»“æœ
        print_results(results)
        
        # ä¿å­˜ç»“æœåˆ°CSV
        df = pd.DataFrame(results)
        df.to_csv('video_dataset_benchmark_results.csv', index=False)
        print(f"\nğŸ’¾ ç»“æœå·²ä¿å­˜åˆ° video_dataset_benchmark_results.csv")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
    finally:
        # æ¸…ç†Rayèµ„æº
        if ray.is_initialized():
            ray.shutdown()

if __name__ == "__main__":
    main() 