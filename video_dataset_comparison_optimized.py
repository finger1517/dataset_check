#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
è§†é¢‘æ•°æ®é›†å¤„ç†æ•ˆèƒ½æ¯”è¾ƒè„šæœ¬ - å¹¶å‘ä¼˜åŒ–ç‰ˆæœ¬
å……åˆ†åˆ©ç”¨Rayã€HuggingFaceå’ŒPyTorchæ•°æ®é›†çš„å¹¶å‘èƒ½åŠ›
"""

import os
import time
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from torchvision import transforms
import ray
from ray.data import Dataset as RayDataset
from datasets import Dataset as HFDataset
import pandas as pd
from typing import List, Tuple, Dict
import psutil
import gc
from multiprocessing import cpu_count
from concurrent.futures import ThreadPoolExecutor
import threading

# è®¾ç½®è§†é¢‘æ•°æ®è·¯å¾„
VIDEO_DATA_PATH = "../video_data"

def find_all_mp4_files(base_path: str) -> List[str]:
    """é€’å½’æŸ¥æ‰¾æ‰€æœ‰mp4æ–‡ä»¶"""
    mp4_files = []
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if file.endswith('.mp4'):
                mp4_files.append(os.path.join(root, file))
    return mp4_files

def extract_frames_from_video(video_path: str, num_frames: int = 16, target_size: Tuple[int, int] = (224, 224)) -> np.ndarray:
    """ä»è§†é¢‘ä¸­å‡åŒ€æå–æŒ‡å®šæ•°é‡çš„å¸§å¹¶è°ƒæ•´å¤§å°"""
    cap = cv2.VideoCapture(video_path)
    total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
    
    if total_frames == 0:
        cap.release()
        return np.zeros((num_frames, target_size[0], target_size[1], 3), dtype=np.uint8)
    
    # è®¡ç®—å‡åŒ€é‡‡æ ·çš„å¸§ç´¢å¼•
    frame_indices = np.linspace(0, total_frames - 1, num_frames, dtype=int)
    frames = []
    
    for frame_idx in frame_indices:
        cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
        ret, frame = cap.read()
        if ret:
            # è°ƒæ•´å¤§å°å¹¶è½¬æ¢é¢œè‰²ç©ºé—´
            frame = cv2.resize(frame, target_size)
            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
            frames.append(frame)
        else:
            # å¦‚æœè¯»å–å¤±è´¥ï¼Œæ·»åŠ é›¶å¸§
            frames.append(np.zeros((target_size[0], target_size[1], 3), dtype=np.uint8))
    
    cap.release()
    return np.array(frames)

# ==================== PyTorch Dataset ä¼˜åŒ–å®ç° ====================
class OptimizedPyTorchVideoDataset(Dataset):
    """ä¼˜åŒ–çš„PyTorchè§†é¢‘æ•°æ®é›†å®ç°"""
    
    def __init__(self, video_paths: List[str], num_frames: int = 16, target_size: Tuple[int, int] = (224, 224)):
        self.video_paths = video_paths
        self.num_frames = num_frames
        self.target_size = target_size
        self.transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
    
    def __len__(self):
        return len(self.video_paths)
    
    def __getitem__(self, idx):
        video_path = self.video_paths[idx]
        frames = extract_frames_from_video(video_path, self.num_frames, self.target_size)
        
        # è½¬æ¢ä¸ºtensorå¹¶åº”ç”¨æ ‡å‡†åŒ–
        frames_tensor = torch.stack([self.transform(frame) for frame in frames])
        
        return {
            'frames': frames_tensor,
            'video_path': video_path,
            'video_name': os.path.basename(video_path)
        }

def collate_fn(batch):
    """è‡ªå®šä¹‰æ‰¹å¤„ç†å‡½æ•°ï¼Œä¼˜åŒ–å†…å­˜ä½¿ç”¨"""
    frames = torch.stack([item['frames'] for item in batch])
    video_paths = [item['video_path'] for item in batch]
    video_names = [item['video_name'] for item in batch]
    
    return {
        'frames': frames,
        'video_paths': video_paths,
        'video_names': video_names
    }

# ==================== Ray Dataset ä¼˜åŒ–å®ç° ====================
def process_video_ray_optimized(batch: Dict) -> Dict:
    """Rayæ•°æ®å¤„ç†å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä½¿ç”¨çº¿ç¨‹æ± """
    video_paths = batch['video_path']
    processed_batch = {
        'frames': [],
        'video_path': [],
        'video_name': []
    }
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œå¤„ç†è§†é¢‘
    with ThreadPoolExecutor(max_workers=4) as executor:
        futures = []
        for video_path in video_paths:
            future = executor.submit(extract_frames_from_video, video_path, 16, (224, 224))
            futures.append((future, video_path))
        
        for future, video_path in futures:
            try:
                frames = future.result()
                # æ ‡å‡†åŒ–å¤„ç†
                frames = frames.astype(np.float32) / 255.0
                frames = (frames - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
                
                processed_batch['frames'].append(frames)
                processed_batch['video_path'].append(video_path)
                processed_batch['video_name'].append(os.path.basename(video_path))
            except Exception as e:
                print(f"å¤„ç†è§†é¢‘ {video_path} æ—¶å‡ºé”™: {str(e)}")
                # æ·»åŠ ç©ºæ•°æ®
                empty_frames = np.zeros((16, 224, 224, 3), dtype=np.float32)
                processed_batch['frames'].append(empty_frames)
                processed_batch['video_path'].append(video_path)
                processed_batch['video_name'].append(os.path.basename(video_path))
    
    return processed_batch

def create_ray_dataset_optimized(video_paths: List[str], num_workers: int = None) -> RayDataset:
    """åˆ›å»ºä¼˜åŒ–çš„Rayæ•°æ®é›†"""
    if num_workers is None:
        num_workers = min(cpu_count(), 8)
    
    data = [{'video_path': path} for path in video_paths]
    ds = ray.data.from_items(data)
    
    # ä½¿ç”¨æ›´å¤§çš„æ‰¹å¤„ç†å¤§å°å’Œæ›´å¤šå¹¶è¡Œåº¦
    return ds.map_batches(
        process_video_ray_optimized, 
        batch_size=8,
        num_cpus=2,  # æ¯ä¸ªä»»åŠ¡ä½¿ç”¨2ä¸ªCPU
        concurrency=num_workers
    )

# ==================== HuggingFace Dataset ä¼˜åŒ–å®ç° ====================
def process_video_hf_optimized(example):
    """HuggingFaceæ•°æ®å¤„ç†å‡½æ•° - ä¼˜åŒ–ç‰ˆæœ¬"""
    video_path = example['video_path']
    
    try:
        frames = extract_frames_from_video(video_path, 16, (224, 224))
        
        # æ ‡å‡†åŒ–å¤„ç†
        frames = frames.astype(np.float32) / 255.0
        frames = (frames - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
        
        return {
            'frames': frames,
            'video_path': video_path,
            'video_name': os.path.basename(video_path)
        }
    except Exception as e:
        print(f"å¤„ç†è§†é¢‘ {video_path} æ—¶å‡ºé”™: {str(e)}")
        empty_frames = np.zeros((16, 224, 224, 3), dtype=np.float32)
        return {
            'frames': empty_frames,
            'video_path': video_path,
            'video_name': os.path.basename(video_path)
        }

def create_hf_dataset_optimized(video_paths: List[str], num_proc: int = None) -> HFDataset:
    """åˆ›å»ºä¼˜åŒ–çš„HuggingFaceæ•°æ®é›†"""
    if num_proc is None:
        num_proc = min(cpu_count(), 8)
    
    data = [{'video_path': path} for path in video_paths]
    dataset = HFDataset.from_list(data)
    
    # ä½¿ç”¨æ›´å¤šè¿›ç¨‹å’Œæ‰¹å¤„ç†
    return dataset.map(
        process_video_hf_optimized, 
        num_proc=num_proc,
        batch_size=1000,  # æ›´å¤§çš„æ‰¹å¤„ç†å¤§å°
        desc="å¤„ç†è§†é¢‘æ•°æ®"
    )

# ==================== æ€§èƒ½æµ‹è¯•å‡½æ•° ====================
def measure_memory_usage():
    """æµ‹é‡å†…å­˜ä½¿ç”¨æƒ…å†µ"""
    process = psutil.Process(os.getpid())
    return process.memory_info().rss / 1024 / 1024  # MB

def benchmark_pytorch_dataset_optimized(video_paths: List[str], batch_size: int = 8, num_workers: int = None) -> Dict:
    """æµ‹è¯•ä¼˜åŒ–çš„PyTorchæ•°æ®é›†æ€§èƒ½"""
    if num_workers is None:
        num_workers = min(cpu_count(), 8)
    
    print(f"ğŸ”¥ æµ‹è¯•ä¼˜åŒ–PyTorchæ•°æ®é›† (workers={num_workers}, batch_size={batch_size})...")
    
    start_memory = measure_memory_usage()
    start_time = time.time()
    
    dataset = OptimizedPyTorchVideoDataset(video_paths)
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=False, 
        num_workers=num_workers,
        collate_fn=collate_fn,
        pin_memory=torch.cuda.is_available(),
        persistent_workers=True if num_workers > 0 else False,
        prefetch_factor=4 if num_workers > 0 else 2
    )
    
    processed_samples = 0
    batch_count = 0
    
    for batch in dataloader:
        batch_count += 1
        processed_samples += len(batch['frames'])
        # æ¨¡æ‹Ÿä¸€äº›å¤„ç†
        _ = batch['frames'].mean()
        
        if batch_count % 10 == 0:
            print(f"   å·²å¤„ç† {processed_samples} ä¸ªæ ·æœ¬...")
    
    end_time = time.time()
    end_memory = measure_memory_usage()
    
    return {
        'name': f'PyTorch Dataset (ä¼˜åŒ–ç‰ˆ, {num_workers}workers)',
        'total_time': end_time - start_time,
        'samples_processed': processed_samples,
        'samples_per_second': processed_samples / (end_time - start_time),
        'memory_usage_mb': end_memory - start_memory,
        'peak_memory_mb': end_memory,
        'num_workers': num_workers,
        'batch_size': batch_size
    }

def benchmark_ray_dataset_optimized(video_paths: List[str], batch_size: int = 8, num_workers: int = None) -> Dict:
    """æµ‹è¯•ä¼˜åŒ–çš„Rayæ•°æ®é›†æ€§èƒ½"""
    if num_workers is None:
        num_workers = min(cpu_count(), 8)
    
    print(f"âš¡ æµ‹è¯•ä¼˜åŒ–Rayæ•°æ®é›† (workers={num_workers}, batch_size={batch_size})...")
    
    start_memory = measure_memory_usage()
    start_time = time.time()
    
    dataset = create_ray_dataset_optimized(video_paths, num_workers)
    
    processed_samples = 0
    batch_count = 0
    
    for batch in dataset.iter_batches(batch_size=batch_size):
        batch_count += 1
        processed_samples += len(batch['frames'])
        # æ¨¡æ‹Ÿä¸€äº›å¤„ç†
        _ = np.mean([np.mean(frames) for frames in batch['frames']])
        
        if batch_count % 10 == 0:
            print(f"   å·²å¤„ç† {processed_samples} ä¸ªæ ·æœ¬...")
    
    end_time = time.time()
    end_memory = measure_memory_usage()
    
    return {
        'name': f'Ray Dataset (ä¼˜åŒ–ç‰ˆ, {num_workers}workers)',
        'total_time': end_time - start_time,
        'samples_processed': processed_samples,
        'samples_per_second': processed_samples / (end_time - start_time),
        'memory_usage_mb': end_memory - start_memory,
        'peak_memory_mb': end_memory,
        'num_workers': num_workers,
        'batch_size': batch_size
    }

def benchmark_hf_dataset_optimized(video_paths: List[str], batch_size: int = 8, num_proc: int = None) -> Dict:
    """æµ‹è¯•ä¼˜åŒ–çš„HuggingFaceæ•°æ®é›†æ€§èƒ½"""
    if num_proc is None:
        num_proc = min(cpu_count(), 8)
    
    print(f"ğŸ¤— æµ‹è¯•ä¼˜åŒ–HuggingFaceæ•°æ®é›† (processes={num_proc}, batch_size={batch_size})...")
    
    start_memory = measure_memory_usage()
    start_time = time.time()
    
    dataset = create_hf_dataset_optimized(video_paths, num_proc)
    
    processed_samples = 0
    batch_count = 0
    
    # ä½¿ç”¨æ›´é«˜æ•ˆçš„æ‰¹å¤„ç†è¿­ä»£
    for i in range(0, len(dataset), batch_size):
        batch_count += 1
        batch = dataset[i:i+batch_size]
        
        if isinstance(batch['frames'], list):
            processed_samples += len(batch['frames'])
            # æ¨¡æ‹Ÿä¸€äº›å¤„ç†
            _ = np.mean([np.mean(frames) for frames in batch['frames']])
        else:
            processed_samples += 1
            _ = np.mean(batch['frames'])
        
        if batch_count % 10 == 0:
            print(f"   å·²å¤„ç† {processed_samples} ä¸ªæ ·æœ¬...")
    
    end_time = time.time()
    end_memory = measure_memory_usage()
    
    return {
        'name': f'HuggingFace Dataset (ä¼˜åŒ–ç‰ˆ, {num_proc}processes)',
        'total_time': end_time - start_time,
        'samples_processed': processed_samples,
        'samples_per_second': processed_samples / (end_time - start_time),
        'memory_usage_mb': end_memory - start_memory,
        'peak_memory_mb': end_memory,
        'num_workers': num_proc,
        'batch_size': batch_size
    }

def run_concurrent_comparison(video_paths: List[str]) -> List[Dict]:
    """è¿è¡Œå¹¶å‘æ€§èƒ½æ¯”è¾ƒæµ‹è¯•"""
    results = []
    
    # æµ‹è¯•ä¸åŒçš„å¹¶å‘é…ç½®
    worker_configs = [2, 4, 8] if cpu_count() >= 8 else [2, 4]
    batch_sizes = [4, 8, 16]
    
    print(f"ğŸš€ å¼€å§‹å¹¶å‘æ€§èƒ½æµ‹è¯• (CPUæ ¸å¿ƒæ•°: {cpu_count()})")
    print(f"æµ‹è¯•é…ç½®: workers={worker_configs}, batch_sizes={batch_sizes}")
    
    for num_workers in worker_configs:
        for batch_size in batch_sizes:
            print(f"\n{'='*60}")
            print(f"æµ‹è¯•é…ç½®: {num_workers} workers, batch_size={batch_size}")
            print(f"{'='*60}")
            
            # æµ‹è¯•PyTorch
            gc.collect()
            pytorch_result = benchmark_pytorch_dataset_optimized(
                video_paths, batch_size=batch_size, num_workers=num_workers
            )
            results.append(pytorch_result)
            
            # æµ‹è¯•Ray
            gc.collect()
            ray_result = benchmark_ray_dataset_optimized(
                video_paths, batch_size=batch_size, num_workers=num_workers
            )
            results.append(ray_result)
            
            # æµ‹è¯•HuggingFace
            gc.collect()
            hf_result = benchmark_hf_dataset_optimized(
                video_paths, batch_size=batch_size, num_proc=num_workers
            )
            results.append(hf_result)
    
    return results

def print_detailed_results(results: List[Dict]):
    """æ‰“å°è¯¦ç»†çš„æµ‹è¯•ç»“æœ"""
    print("\n" + "="*100)
    print("ğŸ“Š è§†é¢‘æ•°æ®é›†å¤„ç†å¹¶å‘æ€§èƒ½æ¯”è¾ƒç»“æœ")
    print("="*100)
    
    # æŒ‰æ¡†æ¶åˆ†ç»„æ˜¾ç¤ºç»“æœ
    frameworks = {}
    for result in results:
        framework = result['name'].split(' ')[0]
        if framework not in frameworks:
            frameworks[framework] = []
        frameworks[framework].append(result)
    
    for framework, framework_results in frameworks.items():
        print(f"\nğŸ¯ {framework} æ¡†æ¶ç»“æœ:")
        print("-" * 80)
        
        for result in framework_results:
            print(f"   é…ç½®: {result['num_workers']}workers, batch={result['batch_size']}")
            print(f"   å¤„ç†æ—¶é—´: {result['total_time']:.2f}ç§’")
            print(f"   å¤„ç†é€Ÿåº¦: {result['samples_per_second']:.2f}æ ·æœ¬/ç§’")
            print(f"   å†…å­˜ä½¿ç”¨: {result['memory_usage_mb']:.2f}MB")
            print()
    
    # æ‰¾å‡ºæœ€ä½³é…ç½®
    fastest = max(results, key=lambda x: x['samples_per_second'])
    most_efficient_memory = min(results, key=lambda x: x['memory_usage_mb'])
    
    print(f"\nğŸ† æœ€å¿«é…ç½®: {fastest['name']}")
    print(f"   å¤„ç†é€Ÿåº¦: {fastest['samples_per_second']:.2f} æ ·æœ¬/ç§’")
    print(f"   é…ç½®: {fastest['num_workers']}workers, batch={fastest['batch_size']}")
    
    print(f"\nğŸ’¾ æœ€çœå†…å­˜é…ç½®: {most_efficient_memory['name']}")
    print(f"   å†…å­˜ä½¿ç”¨: {most_efficient_memory['memory_usage_mb']:.2f} MB")
    print(f"   é…ç½®: {most_efficient_memory['num_workers']}workers, batch={most_efficient_memory['batch_size']}")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸ¬ å¼€å§‹è§†é¢‘æ•°æ®é›†å¤„ç†å¹¶å‘æ•ˆèƒ½æ¯”è¾ƒæµ‹è¯•...")
    print(f"ğŸ’» ç³»ç»Ÿä¿¡æ¯: {cpu_count()} CPUæ ¸å¿ƒ")
    
    # åˆå§‹åŒ–Ray
    if not ray.is_initialized():
        ray.init(
            num_cpus=cpu_count(),
            ignore_reinit_error=True,
            include_dashboard=False
        )
    
    # æŸ¥æ‰¾æ‰€æœ‰è§†é¢‘æ–‡ä»¶
    print("ğŸ“ æ­£åœ¨æœç´¢è§†é¢‘æ–‡ä»¶...")
    video_paths = find_all_mp4_files(VIDEO_DATA_PATH)
    
    if not video_paths:
        print("âŒ æœªæ‰¾åˆ°ä»»ä½•mp4æ–‡ä»¶ï¼")
        return
    
    # é™åˆ¶æµ‹è¯•æ–‡ä»¶æ•°é‡
    test_video_paths = video_paths[:min(200, len(video_paths))]
    print(f"ğŸ“¹ æ‰¾åˆ° {len(video_paths)} ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œå°†æµ‹è¯•å‰ {len(test_video_paths)} ä¸ª")
    
    try:
        # è¿è¡Œå¹¶å‘æ¯”è¾ƒæµ‹è¯•
        results = run_concurrent_comparison(test_video_paths)
        
        # æ‰“å°è¯¦ç»†ç»“æœ
        print_detailed_results(results)
        
        # ä¿å­˜ç»“æœåˆ°CSV
        df = pd.DataFrame(results)
        df.to_csv('video_dataset_concurrent_benchmark_results.csv', index=False)
        print(f"\nğŸ’¾ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° video_dataset_concurrent_benchmark_results.csv")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        import traceback
        traceback.print_exc()
    finally:
        # æ¸…ç†Rayèµ„æº
        if ray.is_initialized():
            ray.shutdown()

if __name__ == "__main__":
    main() 