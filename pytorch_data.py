#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
PyTorchæ•°æ®é›†å®ç°
ä½¿ç”¨PyTorch Datasetå’ŒDataLoaderè¿›è¡Œè§†é¢‘æ•°æ®å¤„ç†
"""

import os
import cv2
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader, IterableDataset
from torchvision import transforms
from typing import List, Dict, Tuple, Iterator
import time
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
import multiprocessing as mp

class PyTorchVideoDataset(Dataset):
    """PyTorchæ ‡å‡†è§†é¢‘æ•°æ®é›†ç±»"""
    
    def __init__(self, video_paths: List[str], num_frames: int = 16, target_size: Tuple[int, int] = (224, 224), 
                 transform=None, preload: bool = False):
        """
        åˆå§‹åŒ–PyTorchè§†é¢‘æ•°æ®é›†
        
        Args:
            video_paths: è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            num_frames: æ¯ä¸ªè§†é¢‘æå–çš„å¸§æ•°
            target_size: ç›®æ ‡å›¾åƒå°ºå¯¸
            transform: æ•°æ®å˜æ¢
            preload: æ˜¯å¦é¢„åŠ è½½æ‰€æœ‰æ•°æ®åˆ°å†…å­˜
        """
        self.video_paths = video_paths
        self.num_frames = num_frames
        self.target_size = target_size
        self.preload = preload
        
        # é»˜è®¤å˜æ¢
        if transform is None:
            self.transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            self.transform = transform
        
        # é¢„åŠ è½½æ•°æ®
        if self.preload:
            print("é¢„åŠ è½½è§†é¢‘æ•°æ®åˆ°å†…å­˜...")
            self.preloaded_data = self._preload_all_videos()
            print(f"é¢„åŠ è½½å®Œæˆï¼Œå…± {len(self.preloaded_data)} ä¸ªè§†é¢‘")
    
    def _extract_frames_from_video(self, video_path: str) -> np.ndarray:
        """ä»è§†é¢‘ä¸­å‡åŒ€æå–å¸§"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames == 0:
            cap.release()
            return np.zeros((self.num_frames, self.target_size[0], self.target_size[1], 3), dtype=np.uint8)
        
        # è®¡ç®—å‡åŒ€é‡‡æ ·çš„å¸§ç´¢å¼•
        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)
        frames = []
        
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                # è°ƒæ•´å¤§å°å¹¶è½¬æ¢é¢œè‰²ç©ºé—´
                frame = cv2.resize(frame, self.target_size)
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame)
            else:
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œæ·»åŠ é›¶å¸§
                frames.append(np.zeros((self.target_size[0], self.target_size[1], 3), dtype=np.uint8))
        
        cap.release()
        return np.array(frames)
    
    def _preload_all_videos(self) -> List[Dict]:
        """é¢„åŠ è½½æ‰€æœ‰è§†é¢‘æ•°æ®"""
        preloaded_data = []
        
        with ProcessPoolExecutor(max_workers=40) as executor:
            futures = []
            for video_path in self.video_paths:
                future = executor.submit(self._extract_frames_from_video, video_path)
                futures.append((future, video_path))
            
            for future, video_path in futures:
                try:
                    frames = future.result()
                    preloaded_data.append({
                        'frames': frames,
                        'video_path': video_path,
                        'video_name': os.path.basename(video_path)
                    })
                except Exception as e:
                    print(f"é¢„åŠ è½½è§†é¢‘ {video_path} æ—¶å‡ºé”™: {str(e)}")
                    empty_frames = np.zeros((self.num_frames, self.target_size[0], self.target_size[1], 3), dtype=np.uint8)
                    preloaded_data.append({
                        'frames': empty_frames,
                        'video_path': video_path,
                        'video_name': os.path.basename(video_path)
                    })
        
        return preloaded_data
    
    def __len__(self):
        return len(self.video_paths)
    
    def __getitem__(self, idx):
        if self.preload:
            # ä½¿ç”¨é¢„åŠ è½½çš„æ•°æ®
            data = self.preloaded_data[idx]
            frames = data['frames']
            video_path = data['video_path']
            video_name = data['video_name']
        else:
            # å®æ—¶åŠ è½½
            video_path = self.video_paths[idx]
            frames = self._extract_frames_from_video(video_path)
            video_name = os.path.basename(video_path)
        
        # è½¬æ¢ä¸ºtensorå¹¶åº”ç”¨å˜æ¢
        frames_tensor = torch.stack([self.transform(frame) for frame in frames])
        
        return {
            'frames': frames_tensor,
            'video_path': video_path,
            'video_name': video_name,
            'video_idx': idx
        }

class PyTorchIterableVideoDataset(IterableDataset):
    """PyTorchå¯è¿­ä»£è§†é¢‘æ•°æ®é›†ç±»ï¼ˆé€‚åˆå¤§è§„æ¨¡æ•°æ®ï¼‰"""
    
    def __init__(self, video_paths: List[str], num_frames: int = 16, target_size: Tuple[int, int] = (224, 224), 
                 transform=None, shuffle: bool = False):
        """
        åˆå§‹åŒ–PyTorchå¯è¿­ä»£è§†é¢‘æ•°æ®é›†
        
        Args:
            video_paths: è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            num_frames: æ¯ä¸ªè§†é¢‘æå–çš„å¸§æ•°
            target_size: ç›®æ ‡å›¾åƒå°ºå¯¸
            transform: æ•°æ®å˜æ¢
            shuffle: æ˜¯å¦æ‰“ä¹±æ•°æ®
        """
        self.video_paths = video_paths
        self.num_frames = num_frames
        self.target_size = target_size
        self.shuffle = shuffle
        
        # é»˜è®¤å˜æ¢
        if transform is None:
            self.transform = transforms.Compose([
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            self.transform = transform
    
    def _extract_frames_from_video(self, video_path: str) -> np.ndarray:
        """ä»è§†é¢‘ä¸­å‡åŒ€æå–å¸§"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames == 0:
            cap.release()
            return np.zeros((self.num_frames, self.target_size[0], self.target_size[1], 3), dtype=np.uint8)
        
        # è®¡ç®—å‡åŒ€é‡‡æ ·çš„å¸§ç´¢å¼•
        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)
        frames = []
        
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                # è°ƒæ•´å¤§å°å¹¶è½¬æ¢é¢œè‰²ç©ºé—´
                frame = cv2.resize(frame, self.target_size)
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame)
            else:
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œæ·»åŠ é›¶å¸§
                frames.append(np.zeros((self.target_size[0], self.target_size[1], 3), dtype=np.uint8))
        
        cap.release()
        return np.array(frames)
    
    def __iter__(self) -> Iterator[Dict]:
        video_paths = self.video_paths.copy()
        
        if self.shuffle:
            np.random.shuffle(video_paths)
        
        for idx, video_path in enumerate(video_paths):
            try:
                frames = self._extract_frames_from_video(video_path)
                
                # è½¬æ¢ä¸ºtensorå¹¶åº”ç”¨å˜æ¢
                frames_tensor = torch.stack([self.transform(frame) for frame in frames])
                
                yield {
                    'frames': frames_tensor,
                    'video_path': video_path,
                    'video_name': os.path.basename(video_path),
                    'video_idx': idx
                }
            except Exception as e:
                print(f"å¤„ç†è§†é¢‘ {video_path} æ—¶å‡ºé”™: {str(e)}")
                # è¿”å›ç©ºæ•°æ®
                empty_frames = np.zeros((self.num_frames, self.target_size[0], self.target_size[1], 3), dtype=np.uint8)
                frames_tensor = torch.stack([self.transform(frame) for frame in empty_frames])
                
                yield {
                    'frames': frames_tensor,
                    'video_path': video_path,
                    'video_name': os.path.basename(video_path),
                    'video_idx': idx
                }

def benchmark_pytorch_dataset_advanced(video_paths: List[str], batch_size: int = 4, num_workers: int = 2, 
                                     preload: bool = False, use_iterable: bool = False) -> Dict:
    """é«˜çº§PyTorchæ•°æ®é›†æ€§èƒ½æµ‹è¯•"""
    dataset_type = "Iterable" if use_iterable else ("Preloaded" if preload else "Standard")
    print(f"ğŸ”¥ å¼€å§‹PyTorchæ•°æ®é›†é«˜çº§æ€§èƒ½æµ‹è¯• ({dataset_type}, batch_size={batch_size}, workers={num_workers})...")
    
    start_time = time.time()
    
    # åˆ›å»ºæ•°æ®é›†
    if use_iterable:
        dataset = PyTorchIterableVideoDataset(video_paths, shuffle=True)
    else:
        dataset = PyTorchVideoDataset(video_paths, preload=preload)
    
    # åˆ›å»ºæ•°æ®åŠ è½½å™¨
    dataloader = DataLoader(
        dataset, 
        batch_size=batch_size, 
        shuffle=not use_iterable,  # å¯è¿­ä»£æ•°æ®é›†å†…éƒ¨å¤„ç†shuffle
        num_workers=num_workers,
        pin_memory=torch.cuda.is_available(),
        persistent_workers=num_workers > 0
    )
    
    processed_samples = 0
    batch_count = 0
    
    # éå†æ•°æ®åŠ è½½å™¨
    for batch in dataloader:
        batch_count += 1
        current_batch_size = len(batch['frames'])
        processed_samples += current_batch_size
        
        # æ¨¡æ‹Ÿä¸€äº›è®¡ç®—
        _ = batch['frames'].mean()
        print(type(batch['frames']))
        
        # # å¦‚æœæœ‰GPUï¼Œæµ‹è¯•GPUä¼ è¾“
        # if torch.cuda.is_available():
        #     gpu_frames = batch['frames'].cuda()
        #     _ = gpu_frames.mean()
        #     del gpu_frames
        
        if batch_count % 5 == 0:
            print(f"   å·²å¤„ç† {processed_samples} ä¸ªæ ·æœ¬...")
    
    end_time = time.time()
    total_time = end_time - start_time
    
    return {
        'name': f'PyTorch Dataset ({dataset_type})',
        'total_time': total_time,
        'samples_processed': processed_samples,
        'samples_per_second': processed_samples / total_time if total_time > 0 else 0,
        'batch_count': batch_count,
        'avg_batch_size': processed_samples / batch_count if batch_count > 0 else 0,
        'num_workers': num_workers,
        'preload': preload,
        'use_iterable': use_iterable,
        'gpu_available': torch.cuda.is_available()
    }

def compare_pytorch_variants(video_paths: List[str], batch_size: int = 4) -> List[Dict]:
    """æ¯”è¾ƒä¸åŒPyTorchæ•°æ®é›†å˜ä½“çš„æ€§èƒ½"""
    results = []
    
    # æµ‹è¯•æ ‡å‡†æ•°æ®é›†
    result1 = benchmark_pytorch_dataset_advanced(
        video_paths, batch_size=batch_size, num_workers=0, preload=False, use_iterable=False
    )
    results.append(result1)
    
    # æµ‹è¯•å¤šè¿›ç¨‹æ•°æ®é›†
    result2 = benchmark_pytorch_dataset_advanced(
        video_paths, batch_size=batch_size, num_workers=2, preload=False, use_iterable=False
    )
    results.append(result2)
    
    # æµ‹è¯•é¢„åŠ è½½æ•°æ®é›†
    result3 = benchmark_pytorch_dataset_advanced(
        video_paths, batch_size=batch_size, num_workers=0, preload=True, use_iterable=False
    )
    results.append(result3)
    
    # æµ‹è¯•å¯è¿­ä»£æ•°æ®é›†
    result4 = benchmark_pytorch_dataset_advanced(
        video_paths, batch_size=batch_size, num_workers=2, preload=False, use_iterable=True
    )
    results.append(result4)
    
    return results

if __name__ == "__main__":
    # æµ‹è¯•PyTorchæ•°æ®é›†å®ç°
    import glob
    
    # æŸ¥æ‰¾æµ‹è¯•è§†é¢‘æ–‡ä»¶
    video_paths = []
    for root, dirs, files in os.walk("../video_data"):
        for file in files:
            if file.endswith('.mp4'):
                video_paths.append(os.path.join(root, file))
    
    if video_paths:
        test_paths = video_paths[:50]  # æµ‹è¯•å‰50ä¸ªè§†é¢‘
        print(f"æ‰¾åˆ° {len(video_paths)} ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œæµ‹è¯•å‰ {len(test_paths)} ä¸ª")
        
        # æ¯”è¾ƒä¸åŒå˜ä½“
        # results = compare_pytorch_variants(test_paths, batch_size=4)
        result = benchmark_pytorch_dataset_advanced(test_paths, batch_size=4, num_workers=40, preload=False, use_iterable=False)
        
        # print("\n" + "="*60)
        # print("PyTorchæ•°æ®é›†å˜ä½“æ€§èƒ½æ¯”è¾ƒç»“æœ")
        # print("="*60)
        
        print(f"\nğŸ¯ {result['name']}:")
        print(f"   æ€»å¤„ç†æ—¶é—´: {result['total_time']:.2f} ç§’")
        print(f"   å¤„ç†æ ·æœ¬æ•°: {result['samples_processed']}")
        print(f"   å¤„ç†é€Ÿåº¦: {result['samples_per_second']:.2f} æ ·æœ¬/ç§’")
        print(f"   å·¥ä½œè¿›ç¨‹æ•°: {result['num_workers']}")
        print(f"   é¢„åŠ è½½: {result['preload']}")
        print(f"   å¯è¿­ä»£: {result['use_iterable']}")
        print(f"   GPUå¯ç”¨: {result['gpu_available']}")
        
        # æ‰¾å‡ºæœ€å¿«çš„å˜ä½“
        # fastest = min(results, key=lambda x: x['total_time'])
        # print(f"\nğŸ† æœ€å¿«å˜ä½“: {fastest['name']} ({fastest['samples_per_second']:.2f} æ ·æœ¬/ç§’)")
        
    else:
        print("æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶è¿›è¡Œæµ‹è¯•") 