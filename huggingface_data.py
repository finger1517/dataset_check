#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
HuggingFaceæ•°æ®é›†å®ç°
ä½¿ç”¨HuggingFace Datasetsåº“è¿›è¡Œè§†é¢‘æ•°æ®å¤„ç†
"""

import os
import cv2
import numpy as np
from datasets import Dataset as HFDataset, Features, Array3D, Value, IterableDataset
from typing import List, Dict, Tuple, Iterator, Any
import time
from multiprocessing import cpu_count
import polars as pl
import torch

class HuggingFaceVideoDataset:
    """HuggingFaceè§†é¢‘æ•°æ®é›†ç±»"""
    
    def __init__(self, video_paths: List[str], num_frames: int = 16, target_size: Tuple[int, int] = (224, 224)):
        """
        åˆå§‹åŒ–HuggingFaceè§†é¢‘æ•°æ®é›†
        
        Args:
            video_paths: è§†é¢‘æ–‡ä»¶è·¯å¾„åˆ—è¡¨
            num_frames: æ¯ä¸ªè§†é¢‘æå–çš„å¸§æ•°
            target_size: ç›®æ ‡å›¾åƒå°ºå¯¸
        """
        self.video_paths = video_paths
        self.num_frames = num_frames
        self.target_size = target_size
    
    def _extract_frames_from_video(self, video_path: str) -> np.ndarray:
        """ä»è§†é¢‘ä¸­å‡åŒ€æå–å¸§"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames == 0:
            cap.release()
            return np.zeros((self.num_frames, self.target_size[0], self.target_size[1], 3), dtype=np.uint8)
        
        # è®¡ç®—å‡åŒ€é‡‡æ ·çš„å¸§ç´¢å¼•
        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)
        frames = []
        
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                # è°ƒæ•´å¤§å°å¹¶è½¬æ¢é¢œè‰²ç©ºé—´
                frame = cv2.resize(frame, self.target_size)
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame)
            else:
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œæ·»åŠ é›¶å¸§
                frames.append(np.zeros((self.target_size[0], self.target_size[1], 3), dtype=np.uint8))
        
        cap.release()
        return np.array(frames)
    
    def _process_video_example(self, example):
        """å¤„ç†å•ä¸ªè§†é¢‘æ ·æœ¬"""
        video_path = example['video_path']
        
        try:
            frames = self._extract_frames_from_video(video_path)
            
            # æ ‡å‡†åŒ–å¤„ç†
            frames = frames.astype(np.float32) / 255.0
            frames = (frames - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
            
            return {
                'frames': frames,
                'video_path': video_path,
                'video_name': os.path.basename(video_path),
                'frame_count': len(frames),
                'video_size': os.path.getsize(video_path) if os.path.exists(video_path) else 0
            }
            
        except Exception as e:
            print(f"å¤„ç†è§†é¢‘ {video_path} æ—¶å‡ºé”™: {str(e)}")
            # è¿”å›ç©ºæ•°æ®
            empty_frames = np.zeros((self.num_frames, self.target_size[0], self.target_size[1], 3), dtype=np.float32)
            return {
                'frames': empty_frames,
                'video_path': video_path,
                'video_name': os.path.basename(video_path),
                'frame_count': 0,
                'video_size': 0
            }
    
    def create_dataset(self, num_proc: int = None, cache_dir: str = None) -> HFDataset:
        """åˆ›å»ºHuggingFaceæ•°æ®é›†"""
        if num_proc is None:
            num_proc = min(4, cpu_count())
        
        # åˆ›å»ºåˆå§‹æ•°æ®
        data = [{'video_path': path} for path in self.video_paths]
        
        # å®šä¹‰æ•°æ®é›†ç‰¹å¾
        features = Features({
            'frames': Array3D(shape=(self.num_frames, self.target_size[0], self.target_size[1], 3), dtype='float32'),
            'video_path': Value('string'),
            'video_name': Value('string'),
            'frame_count': Value('int32'),
            'video_size': Value('int64')
        })
        
        # åˆ›å»ºHuggingFaceæ•°æ®é›†
        dataset = HFDataset.from_list(data)
        
        # åº”ç”¨å¤„ç†å‡½æ•°
        processed_dataset = dataset.map(
            self._process_video_example,
            num_proc=num_proc,
            # batched=True,
            # batch_size=4,
            # features=features,
            # cache_file_name=f"{cache_dir}/processed_videos.arrow" if cache_dir else None,
            desc="å¤„ç†è§†é¢‘æ•°æ®"
        ).to_iterable_dataset()
        
        return processed_dataset
    
    def get_stats(self) -> Dict:
        """è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'total_videos': len(self.video_paths),
            'frames_per_video': self.num_frames,
            'target_size': self.target_size,
            'total_frames': len(self.video_paths) * self.num_frames
        }

def benchmark_hf_dataset_advanced(video_paths: List[str], batch_size: int = 4, num_proc: int = 4) -> Dict:
    """é«˜çº§HuggingFaceæ•°æ®é›†æ€§èƒ½æµ‹è¯•"""
    print(f"ğŸ¤— å¼€å§‹HuggingFaceæ•°æ®é›†é«˜çº§æ€§èƒ½æµ‹è¯• (batch_size={batch_size}, num_proc={num_proc})...")
    
    start_time = time.time()
    
    # åˆ›å»ºHuggingFaceè§†é¢‘æ•°æ®é›†
    # hf_dataset = HuggingFaceVideoDataset(video_paths)
    # dataset = hf_dataset.create_dataset(num_proc=num_proc)    
    hf_dataset = StreamingVideoDataset(video_paths)
    dataset = hf_dataset.create_streaming_dataset()
    processed_samples = 0
    batch_count = 0
    
    # HuggingFace Datasets æ”¯æŒ batch æ–¹å¼å¤„ç†ï¼Œå¯ä»¥é€šè¿‡ map çš„ batched=True å‚æ•°å®ç°æ‰¹é‡å¤„ç†
    # è¿™é‡Œæ¼”ç¤ºå¦‚ä½•ç”¨ batch æ–¹å¼éå†æ•°æ®é›†
    for batch in dataset.iter(batch_size=batch_size):
        batch_count += 1

        # HuggingFace çš„ batch æ˜¯å­—å…¸ï¼Œæ¯ä¸ª key å¯¹åº”ä¸€ä¸ª list
        current_batch_size = len(batch['frames'])
        processed_samples += current_batch_size
        
        # for frames in batch['frames']:
        #     _ = np.mean(frames)

        # print(type(batch['frames']))

        # ä½¿ç”¨polarsè¿›è¡Œæ‰¹é‡è®¡ç®—
        # frames_df = pl.DataFrame({'frames': batch['frames']})
        # _ = frames_df.select(pl.col('frames').mean())

        if batch_count % 5 == 0:
            print(f"   å·²å¤„ç† {processed_samples} ä¸ªæ ·æœ¬...")
    end_time = time.time()
    total_time = end_time - start_time
    
    # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
    # stats = hf_dataset.get_stats()
    
    return {
        'name': f'HuggingFace Dataset (Advanced)',
        'total_time': total_time,
        'samples_processed': processed_samples,
        'samples_per_second': processed_samples / total_time if total_time > 0 else 0,
        'batch_count': batch_count,
        'avg_batch_size': processed_samples / batch_count if batch_count > 0 else 0,
        # 'dataset_stats': stats,
        # 'dataset_size_mb': dataset.data.nbytes / (1024 * 1024) if hasattr(dataset.data, 'nbytes') else 0
    }

def create_hf_dataset_with_caching(video_paths: List[str], cache_dir: str = "./hf_cache") -> HFDataset:
    """åˆ›å»ºå¸¦ç¼“å­˜çš„HuggingFaceæ•°æ®é›†"""
    os.makedirs(cache_dir, exist_ok=True)
    
    hf_dataset = HuggingFaceVideoDataset(video_paths)
    dataset = hf_dataset.create_dataset(num_proc=4, cache_dir=cache_dir)
    
    # ä¿å­˜æ•°æ®é›†åˆ°ç£ç›˜
    dataset.save_to_disk(f"{cache_dir}/video_dataset")
    print(f"æ•°æ®é›†å·²ä¿å­˜åˆ° {cache_dir}/video_dataset")
    
    return dataset

def load_hf_dataset_from_cache(cache_dir: str = "./hf_cache") -> HFDataset:
    """ä»ç¼“å­˜åŠ è½½HuggingFaceæ•°æ®é›†"""
    dataset_path = f"{cache_dir}/video_dataset"
    if os.path.exists(dataset_path):
        dataset = HFDataset.load_from_disk(dataset_path)
        print(f"ä»ç¼“å­˜åŠ è½½æ•°æ®é›†: {dataset_path}")
        return dataset
    else:
        raise FileNotFoundError(f"ç¼“å­˜æ•°æ®é›†ä¸å­˜åœ¨: {dataset_path}")

class OptimizedHuggingFaceVideoDataset:
    """ä¼˜åŒ–çš„HuggingFaceè§†é¢‘æ•°æ®é›†ï¼Œæ”¯æŒé«˜æ•ˆçš„tensorè¾“å‡º"""
    
    def __init__(self, video_paths: List[str], num_frames: int = 16, target_size: Tuple[int, int] = (224, 224)):
        self.video_paths = video_paths
        self.num_frames = num_frames
        self.target_size = target_size
        
    def _extract_frames_from_video(self, video_path: str) -> np.ndarray:
        """ä»è§†é¢‘ä¸­å‡åŒ€æå–å¸§"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames == 0:
            cap.release()
            return np.zeros((self.num_frames, self.target_size[0], self.target_size[1], 3), dtype=np.uint8)
        
        # è®¡ç®—å‡åŒ€é‡‡æ ·çš„å¸§ç´¢å¼•
        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)
        frames = []
        
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                # è°ƒæ•´å¤§å°å¹¶è½¬æ¢é¢œè‰²ç©ºé—´
                frame = cv2.resize(frame, self.target_size)
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame)
            else:
                # å¦‚æœè¯»å–å¤±è´¥ï¼Œæ·»åŠ é›¶å¸§
                frames.append(np.zeros((self.target_size[0], self.target_size[1], 3), dtype=np.uint8))
        
        cap.release()
        return np.array(frames)

    
    def _process_video_example_optimized(self, example):
        """ä¼˜åŒ–çš„è§†é¢‘å¤„ç†ï¼šä½¿ç”¨numpyé¿å…tensoråºåˆ—åŒ–å¼€é”€"""
        video_path = example['video_path']
        
        try:
            frames = self._extract_frames_from_video(video_path)
            
            # æ ‡å‡†åŒ–å¤„ç† - ä¿æŒnumpyæ ¼å¼
            frames = frames.astype(np.float32) / 255.0
            frames = (frames - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
            
            return {
                'frames': frames,  # numpyæ ¼å¼ï¼Œåºåˆ—åŒ–æ•ˆç‡æ›´é«˜
                'video_path': video_path,
                'video_name': os.path.basename(video_path),
                'frame_count': len(frames),
                'video_size': os.path.getsize(video_path) if os.path.exists(video_path) else 0
            }
            
        except Exception as e:
            print(f"å¤„ç†è§†é¢‘ {video_path} æ—¶å‡ºé”™: {str(e)}")
            empty_frames = np.zeros((self.num_frames, self.target_size[0], self.target_size[1], 3), dtype=np.float32)
            return {
                'frames': empty_frames,
                'video_path': video_path,
                'video_name': os.path.basename(video_path),
                'frame_count': 0,
                'video_size': 0
            }
    
    def create_torch_dataset(self, num_proc: int = None, cache_dir: str = None) -> HFDataset:
        """åˆ›å»ºä¼˜åŒ–çš„torchæ ¼å¼æ•°æ®é›†"""
        if num_proc is None:
            num_proc = min(4, cpu_count())
        
        data = [{'video_path': path} for path in self.video_paths]
        dataset = HFDataset.from_list(data)
        
        # æ­¥éª¤1: ä½¿ç”¨numpyå¤„ç†æ•°æ®ï¼ˆé«˜æ•ˆåºåˆ—åŒ–ï¼‰
        processed_dataset = dataset.map(
            self._process_video_example_optimized,
            num_proc=num_proc,
            desc="å¤„ç†è§†é¢‘æ•°æ®"
        )
        
        # æ­¥éª¤2: è®¾ç½®è¾“å‡ºæ ¼å¼ä¸ºtorchï¼ˆè¿è¡Œæ—¶è½¬æ¢ï¼‰
        torch_dataset = processed_dataset.with_format("torch", columns=["frames"])
        
        return torch_dataset

# ä½¿ç”¨ç¤ºä¾‹
def benchmark_optimized_torch_dataset(video_paths: List[str], batch_size: int = 4, num_proc: int = 4) -> Dict:
    """æµ‹è¯•ä¼˜åŒ–çš„torchæ ¼å¼æ•°æ®é›†"""
    print(f"ğŸš€ æµ‹è¯•ä¼˜åŒ–torchæ ¼å¼HuggingFaceæ•°æ®é›† (batch_size={batch_size}, num_proc={num_proc})...")
    
    start_time = time.time()
    
    # åˆ›å»ºä¼˜åŒ–çš„torchæ•°æ®é›†
    hf_dataset = OptimizedHuggingFaceVideoDataset(video_paths)
    dataset = hf_dataset.create_torch_dataset(num_proc=num_proc)
    
    processed_samples = 0
    batch_count = 0
    
    for batch in dataset.iter(batch_size=batch_size):
        batch_count += 1
        current_batch_size = len(batch['frames'])
        processed_samples += current_batch_size
        
        # éªŒè¯æ•°æ®ç±»å‹
        print(type(batch['frames']))
        # frames = batch['frames'][0]
        # print(f"æ•°æ®ç±»å‹: {type(frames)}, shape: {frames.shape}")  # torch.Tensor
        
        # å¯ä»¥ç›´æ¥ä½¿ç”¨torchæ“ä½œ
        # _ = torch.mean(frames)  # æ— éœ€ç±»å‹è½¬æ¢
        
        if batch_count % 5 == 0:
            print(f"   å·²å¤„ç† {processed_samples} ä¸ªæ ·æœ¬...")
    
    end_time = time.time()
    total_time = end_time - start_time
    
    return {
        'name': 'HuggingFace Dataset (Optimized Torch)',
        'total_time': total_time,
        'samples_processed': processed_samples,
        'samples_per_second': processed_samples / total_time if total_time > 0 else 0,
        'batch_count': batch_count,
        'avg_batch_size': processed_samples / batch_count if batch_count > 0 else 0
    }

class StreamingVideoDataset:
    """æµå¼è§†é¢‘æ•°æ®é›† - é¿å…é¢„å…ˆå¤„ç†æ‰€æœ‰æ•°æ®"""
    
    def __init__(self, video_paths: List[str], num_frames: int = 16, target_size: Tuple[int, int] = (224, 224)):
        self.video_paths = video_paths
        self.num_frames = num_frames
        self.target_size = target_size
    
    def _extract_frames_from_video(self, video_path: str) -> np.ndarray:
        """ä»è§†é¢‘ä¸­æå–å¸§ï¼ˆä¸ä¹‹å‰ç›¸åŒçš„å®ç°ï¼‰"""
        cap = cv2.VideoCapture(video_path)
        total_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        
        if total_frames == 0:
            cap.release()
            return np.zeros((self.num_frames, self.target_size[0], self.target_size[1], 3), dtype=np.uint8)
        
        frame_indices = np.linspace(0, total_frames - 1, self.num_frames, dtype=int)
        frames = []
        
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                frame = cv2.resize(frame, self.target_size)
                frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frames.append(frame)
            else:
                frames.append(np.zeros((self.target_size[0], self.target_size[1], 3), dtype=np.uint8))
        
        cap.release()
        return np.array(frames)
    
    def video_generator(self) -> Iterator[Dict[str, Any]]:
        """è§†é¢‘æ•°æ®ç”Ÿæˆå™¨ - å®æ—¶å¤„ç†ï¼Œæ— é¢„å­˜å‚¨"""
        for video_path in self.video_paths:
            try:
                # å®æ—¶å¤„ç†è§†é¢‘
                frames = self._extract_frames_from_video(video_path)
                
                # æ ‡å‡†åŒ–å¤„ç†
                frames = frames.astype(np.float32) / 255.0
                frames = (frames - np.array([0.485, 0.456, 0.406])) / np.array([0.229, 0.224, 0.225])
                
                yield {
                    'frames': frames,
                    'video_path': video_path,
                    'video_name': os.path.basename(video_path),
                    'frame_count': len(frames),
                    'video_size': os.path.getsize(video_path) if os.path.exists(video_path) else 0
                }
                
            except Exception as e:
                print(f"å¤„ç†è§†é¢‘ {video_path} æ—¶å‡ºé”™: {str(e)}")
                # è¿”å›ç©ºæ•°æ®
                empty_frames = np.zeros((self.num_frames, self.target_size[0], self.target_size[1], 3), dtype=np.float32)
                yield {
                    'frames': empty_frames,
                    'video_path': video_path,
                    'video_name': os.path.basename(video_path),
                    'frame_count': 0,
                    'video_size': 0
                }
    
    def create_streaming_dataset(self) -> IterableDataset:
        """åˆ›å»ºæµå¼æ•°æ®é›†"""
        # ğŸ”¥ å…³é”®ï¼šä½¿ç”¨ç”Ÿæˆå™¨åˆ›å»ºå¯è¿­ä»£æ•°æ®é›†
        iterable_dataset = IterableDataset.from_generator(
            self.video_generator,
            # å¯é€‰ï¼šå®šä¹‰ç‰¹å¾ç±»å‹ï¼ˆç”¨äºç±»å‹æ£€æŸ¥å’Œä¼˜åŒ–ï¼‰
            # features=Features({
            #     'frames': Array3D(shape=(self.num_frames, self.target_size[0], self.target_size[1], 3), dtype='float32'),
            #     'video_path': Value('string'),
            #     'video_name': Value('string'),
            #     'frame_count': Value('int32'),
            #     'video_size': Value('int64')
            # })
        )
        return iterable_dataset

if __name__ == "__main__":
    # æµ‹è¯•HuggingFaceæ•°æ®é›†å®ç°
    import glob
    
    # æŸ¥æ‰¾æµ‹è¯•è§†é¢‘æ–‡ä»¶
    video_paths = []
    for root, dirs, files in os.walk("../video_data"):
        for file in files:
            if file.endswith('.mp4'):
                video_paths.append(os.path.join(root, file))
    
    if video_paths:
        test_paths = video_paths[:50]  # æµ‹è¯•å‰50ä¸ªè§†é¢‘
        print(f"æ‰¾åˆ° {len(video_paths)} ä¸ªè§†é¢‘æ–‡ä»¶ï¼Œæµ‹è¯•å‰ {len(test_paths)} ä¸ª")
        
        # è¿è¡Œæµ‹è¯•
        result = benchmark_hf_dataset_advanced(test_paths, batch_size=4, num_proc=40)
        # result = benchmark_optimized_torch_dataset(test_paths, batch_size=4, num_proc=40)
        
        print("\næµ‹è¯•ç»“æœ:")
        for key, value in result.items():
            print(f"  {key}: {value}")
            
        # æµ‹è¯•ç¼“å­˜åŠŸèƒ½
        print("\næµ‹è¯•ç¼“å­˜åŠŸèƒ½...")
        try:
            cached_dataset = create_hf_dataset_with_caching(test_paths[:3])
            print(f"ç¼“å­˜æ•°æ®é›†å¤§å°: {len(cached_dataset)}")
            
            # å°è¯•ä»ç¼“å­˜åŠ è½½
            loaded_dataset = load_hf_dataset_from_cache()
            print(f"ä»ç¼“å­˜åŠ è½½çš„æ•°æ®é›†å¤§å°: {len(loaded_dataset)}")
            
        except Exception as e:
            print(f"ç¼“å­˜æµ‹è¯•å‡ºé”™: {str(e)}")
    else:
        print("æœªæ‰¾åˆ°è§†é¢‘æ–‡ä»¶è¿›è¡Œæµ‹è¯•") 